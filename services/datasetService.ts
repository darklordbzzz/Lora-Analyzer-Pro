
import { LLMModel, DatasetItem, TrainingLabState } from '../types';
import * as vlm from './vlmService';
import { fileToBase64, createThumbnail } from './fileService';

/**
 * Generate semantic tags and captions for LoRA training
 */
export const tagImageForDataset = async (file: File, model: LLMModel): Promise<{ tags: string[], caption: string }> => {
    const base64 = await fileToBase64(file);
    const prompt = `Analyze this image for LoRA training. Provide a descriptive caption and a comma-separated list of tags (Danbooru style). Return JSON: caption (string), tags (array). Focus on textures, style, and subject specifics.`;
    
    const text = await vlm.executeVLMAction(model, prompt, {
        json: true,
        images: [{ data: base64, mimeType: file.type }]
    });

    try {
        const cleaned = typeof text === 'string' ? text.replace(/```json|```/g, '').trim() : JSON.stringify(text);
        return JSON.parse(cleaned);
    } catch (e) {
        return { caption: "Asset scan complete.", tags: ["training", "unknown_subject"] };
    }
};

/**
 * Packs metadata into a Safetensors header structure
 */
export const createSafetensorsContainer = async (items: DatasetItem[], config: TrainingLabState): Promise<Blob> => {
    const metadata: Record<string, string> = {
        "ss_base_model": config.baseModel,
        "ss_training_type": config.trainingType,
        "ss_optimizer": config.optimizer,
        "ss_network_rank": config.rank.toString(),
        "ss_network_alpha": config.alpha.toString(),
        "ss_total_images": items.length.toString(),
        "ss_tag_frequency": JSON.stringify(calculateTagFrequency(items)),
        "modelspec.title": `Forge Export: ${config.trainingType}`,
        "modelspec.description": "Container generated by Neural Intelligence Hub Forge"
    };

    // The header size (8 bytes uint64) + header JSON + some dummy weight placeholder
    // Safetensors format: [8 bytes header size][JSON header data][Binary Tensors]
    const headerObj: any = { "__metadata__": metadata };
    // For a real safetensor we'd need weights, but for dataset storage we use it as a metadata container
    headerObj["dummy_weight"] = { dtype: "F32", shape: [1, 1], data_offsets: [0, 4] };
    
    const headerStr = JSON.stringify(headerObj);
    const headerBytes = new TextEncoder().encode(headerStr);
    const headerSize = headerBytes.byteLength;

    const buffer = new ArrayBuffer(8 + headerSize + 4);
    const view = new DataView(buffer);
    view.setBigUint64(0, BigInt(headerSize), true);
    
    const uint8 = new Uint8Array(buffer);
    uint8.set(headerBytes, 8);
    
    return new Blob([buffer], { type: "application/octet-stream" });
};

const calculateTagFrequency = (items: DatasetItem[]) => {
    const freq: Record<string, number> = {};
    items.forEach(item => {
        item.tags.forEach(t => {
            freq[t] = (freq[t] || 0) + 1;
        });
    });
    return freq;
};
